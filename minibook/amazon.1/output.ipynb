{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at 'In [5]'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.000562,
     "end_time": "2018-03-31T12:39:04.894409",
     "exception": false,
     "start_time": "2018-03-31T12:39:04.893847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.265467,
     "end_time": "2018-03-31T12:39:05.167371",
     "exception": false,
     "start_time": "2018-03-31T12:39:04.901904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "from my_measures import BinaryClassificationPerformance\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 7e-06,
     "end_time": "2018-03-31T12:39:05.167469",
     "exception": false,
     "start_time": "2018-03-31T12:39:05.167462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Read raw test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.395463,
     "end_time": "2018-03-31T12:39:05.569601",
     "exception": false,
     "start_time": "2018-03-31T12:39:05.174138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91000, 14)\n"
     ]
    }
   ],
   "source": [
    "amazon = pd.read_csv('/home/wolfm2/amazon_data/raw_data_test.csv')\n",
    "print(amazon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.009325,
     "end_time": "2018-03-31T12:39:05.579018",
     "exception": false,
     "start_time": "2018-03-31T12:39:05.569693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 0.1      Id   ProductId          UserId  \\\n",
      "0      264540        182243  182244  B005OSC218   AJZXTZ3I9A84H   \n",
      "1      146085        548350  548351  B004VLVD50  A39RNG8DUXG7ZM   \n",
      "2      281714        539422  539423  B001BDDT8K  A2FRFAQCWZJT3Q   \n",
      "3      280559        119141  119142  B001U0ON5M  A1YREIUL7VDQ0B   \n",
      "4      236948        352256  352257  B000E63L8S  A3JKXNOLX2QRJ7   \n",
      "\n",
      "                         ProfileName  HelpfulnessNumerator  \\\n",
      "0                             Jeromy                     1   \n",
      "1                         W. Chapman                     4   \n",
      "2        B. Davis \"The Happy Hermit\"                     1   \n",
      "3   Jennifer U. Heston \"ex-academic\"                     0   \n",
      "4  darlene conright \"alaskan darlin\"                     2   \n",
      "\n",
      "   HelpfulnessDenominator  Score        Time                  Summary  \\\n",
      "0                       1      5  1316995200                 The Best   \n",
      "1                       4      5  1331596800  Makes great dog bones !   \n",
      "2                       1      5  1228608000            Got Diarrhea?   \n",
      "3                       0      1  1350864000               Too runny!   \n",
      "4                       2      5  1163203200                Great tea   \n",
      "\n",
      "                                                Text  helpScore  helpful  \n",
      "0  I've impressed so many of my friends by bustin...        1.0    False  \n",
      "1  I have a hard time finding the best ingredient...        1.0     True  \n",
      "2  This is an edit so I was unable to change my s...        1.0    False  \n",
      "3  This product does not work well at all.  We bo...        NaN    False  \n",
      "4  I have a cup of peppermint tea most mornings a...        1.0    False  \n",
      "0.0723956043956\n"
     ]
    }
   ],
   "source": [
    "print(amazon.head())\n",
    "print(amazon['helpful'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 8e-06,
     "end_time": "2018-03-31T12:39:05.579235",
     "exception": false,
     "start_time": "2018-03-31T12:39:05.579227",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature extraction on natural language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.008579,
     "end_time": "2018-03-31T12:39:05.595081",
     "exception": false,
     "start_time": "2018-03-31T12:39:05.586502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# corpus = amazon.Text.as_matrix()\n",
    "# X_bag_of_words = vectorizer.fit_transform(corpus)\n",
    "# print(X_bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.133658,
     "end_time": "2018-03-31T12:39:05.728773",
     "exception": true,
     "start_time": "2018-03-31T12:39:05.595115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hv0.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-17ef42bcac6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# look at the text tokenizer=LemmaTokenizer(), strip_accents=ascii,  stop_words={'english'},\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhv0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hv0.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mX_hv0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhv0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamazon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# mw adds uid as token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hv0.pkl'"
     ]
    }
   ],
   "source": [
    "# vectorize Bag of Words from review text; as sparse matrix\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# hv = HashingVectorizer(n_features=2 ** 17, non_negative=True)\n",
    "# hv = joblib.load('hv.pkl')\n",
    "# X_hv = hv.transform(amazon.Text)\n",
    "# print(X_hv.shape)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# look at the text tokenizer=LemmaTokenizer(), strip_accents=ascii,  stop_words={'english'}, \n",
    "hv0 = joblib.load('hv0.pkl')\n",
    "\n",
    "X_hv0 = hv0.fit_transform(amazon.Text) # mw adds uid as token\n",
    "\n",
    "amazon['ScoreX'] = amazon['Score'].apply(lambda x: str(x))  # converts to day of week\n",
    "hv2 = joblib.load('hv2.pkl')\n",
    "X_hv2 = hv2.fit_transform(amazon.ScoreX) # mw adds uid as token\n",
    "\n",
    "X_hv = X_hv0\n",
    "print(X_hv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# transformer = TfidfTransformer()\n",
    "transformer = joblib.load('transformer.pkl')\n",
    "X_tfidf = transformer.transform(X_hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(X_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Create additional quantitative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features from Amazon.csv to add to feature set\n",
    "# amazon['reviewLen'] = amazon['Text'].str.len()\n",
    "\n",
    "# X_quant_features = amazon[[\"Score\", \"reviewLen\"]]\n",
    "X_quant_features = amazon[[]]\n",
    "print(X_quant_features.head(10))\n",
    "print(type(X_quant_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Combine all quantitative features into a single sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "X_quant_features_csr = csr_matrix(X_quant_features)\n",
    "X_combined = hstack([X_tfidf, X_quant_features_csr, X_hv2])\n",
    "X_matrix = csr_matrix(X_combined) # convert to sparse matrix\n",
    "print(X_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Create `X`, scaled matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler(with_mean=False)\n",
    "sc = joblib.load('sc.pkl')\n",
    "X = sc.transform(X_matrix)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### create `y`, vector of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = amazon['helpful'].values\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### predict `y` with the fit on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL: SVM, linear\n",
    "# from sklearn import linear_model\n",
    "# svm = linear_model.SGDClassifier()\n",
    "# svm.fit(X, y)\n",
    "# joblib.dump(svm, 'svm.pkl') # pickle\n",
    "svm = joblib.load('svm.best.pkl') # pickle\n",
    "\n",
    "svm_performance = BinaryClassificationPerformance(svm.predict(X), y, 'svm')\n",
    "svm_performance.compute_measures()\n",
    "print(svm_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL: logistic regression\n",
    "# from sklearn import linear_model\n",
    "# lgs = linear_model.SGDClassifier(loss='log', n_iter=50, alpha=0.00001)\n",
    "# lgs.fit(X, y)\n",
    "# joblib.dump(lgs, 'lgs.pkl') # pickle\n",
    "lgs = joblib.load('lgs.best.pkl')\n",
    "\n",
    "lgs_performance = BinaryClassificationPerformance(lgs.predict(X), y, 'lgs')\n",
    "lgs_performance.compute_measures()\n",
    "print(lgs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL: Naive Bayes\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# nbs = MultinomialNB()\n",
    "# nbs.fit(X, y)\n",
    "# joblib.dump(nbs, 'nbs.pkl') # pickle\n",
    "nbs = joblib.load('nbs.best.pkl')\n",
    "\n",
    "nbs_performance = BinaryClassificationPerformance(nbs.predict(X), y, 'nbs')\n",
    "nbs_performance.compute_measures()\n",
    "print(nbs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL: Ridge Regression Classifier\n",
    "# from sklearn import linear_model\n",
    "# rdg = linear_model.RidgeClassifier()\n",
    "# rdg.fit(X, y)\n",
    "# joblib.dump(rdg, 'rdg.pkl') # pickle\n",
    "rdg = joblib.load('rdg.best.pkl')\n",
    "\n",
    "rdg_performance = BinaryClassificationPerformance(rdg.predict(X), y, 'rdg')\n",
    "rdg_performance.compute_measures()\n",
    "print(rdg_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL: Perceptron\n",
    "# from sklearn import linear_model\n",
    "# prc = linear_model.SGDClassifier(loss='perceptron')\n",
    "# prc.fit(X, y)\n",
    "# joblib.dump(prc, 'prc.pkl') # pickle\n",
    "prc = joblib.load('prc.best.pkl')\n",
    "\n",
    "prc_performance = BinaryClassificationPerformance(prc.predict(X), y, 'prc')\n",
    "prc_performance.compute_measures()\n",
    "print(prc_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### ROC plot to compare performance of various models and fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fits = [svm_performance, lgs_performance, nbs_performance, rdg_performance, prc_performance]\n",
    "\n",
    "for fit in fits:\n",
    "    plt.plot(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], 'bo')\n",
    "    plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], fit.desc)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.title('ROC plot: test set')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "papermill": {
   "duration": 2.317253,
   "end_time": "2018-03-31T12:39:06.459383",
   "environment_variables": {},
   "exception": true,
   "output_path": "output.ipynb",
   "parameters": null,
   "start_time": "2018-03-31T12:39:04.142130",
   "version": "0.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}