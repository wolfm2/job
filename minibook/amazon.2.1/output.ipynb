{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at 'In [17]'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.00048,
     "end_time": "2018-04-11T12:20:15.124527",
     "exception": false,
     "start_time": "2018-04-11T12:20:15.124047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.356202,
     "end_time": "2018-04-11T12:20:15.489777",
     "exception": false,
     "start_time": "2018-04-11T12:20:15.133575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.013923,
     "end_time": "2018-04-11T12:20:15.503792",
     "exception": false,
     "start_time": "2018-04-11T12:20:15.489869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.system('ps aux | grep wolfm2')\n",
    "#os.system('killall -s SIGKILL -u wolfm2')\n",
    "#os.system('cp /home/wolfm2/job.sh .; echo test 1>&2') #; cp ../job.log ../jerbb.txt')\n",
    "\n",
    "isTraining = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 6e-06,
     "end_time": "2018-04-11T12:20:15.503838",
     "exception": false,
     "start_time": "2018-04-11T12:20:15.503832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Read raw training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.642961,
     "end_time": "2018-04-11T12:20:16.155593",
     "exception": false,
     "start_time": "2018-04-11T12:20:15.512632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136500, 14)\n"
     ]
    }
   ],
   "source": [
    "if isTraining:\n",
    "    amazon = pd.read_csv('/home/wolfm2/amazon_data.0/raw_data_train.csv')\n",
    "    #amazon = pd.read_csv('/home/eydu/amazon_data/raw_data_train.csv')\n",
    "    #amazon = pd.read_csv('/home/ich/amazon_data/raw_data_train.csv')\n",
    "else:\n",
    "    amazon = pd.read_csv('/home/wolfm2/amazon_data.0/raw_data_test.csv')\n",
    "\n",
    "print(amazon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.011288,
     "end_time": "2018-04-11T12:20:16.166972",
     "exception": false,
     "start_time": "2018-04-11T12:20:16.155684",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 0.1      Id   ProductId          UserId  \\\n",
      "0      264540        182243  182244  B005OSC218   AJZXTZ3I9A84H   \n",
      "1      146085        548350  548351  B004VLVD50  A39RNG8DUXG7ZM   \n",
      "2      281714        539422  539423  B001BDDT8K  A2FRFAQCWZJT3Q   \n",
      "3      280559        119141  119142  B001U0ON5M  A1YREIUL7VDQ0B   \n",
      "4      236948        352256  352257  B000E63L8S  A3JKXNOLX2QRJ7   \n",
      "\n",
      "                         ProfileName  HelpfulnessNumerator  \\\n",
      "0                             Jeromy                     1   \n",
      "1                         W. Chapman                     4   \n",
      "2        B. Davis \"The Happy Hermit\"                     1   \n",
      "3   Jennifer U. Heston \"ex-academic\"                     0   \n",
      "4  darlene conright \"alaskan darlin\"                     2   \n",
      "\n",
      "   HelpfulnessDenominator  Score        Time                  Summary  \\\n",
      "0                       1      5  1316995200                 The Best   \n",
      "1                       4      5  1331596800  Makes great dog bones !   \n",
      "2                       1      5  1228608000            Got Diarrhea?   \n",
      "3                       0      1  1350864000               Too runny!   \n",
      "4                       2      5  1163203200                Great tea   \n",
      "\n",
      "                                                Text  helpScore  helpful  \n",
      "0  I've impressed so many of my friends by bustin...        1.0    False  \n",
      "1  I have a hard time finding the best ingredient...        1.0     True  \n",
      "2  This is an edit so I was unable to change my s...        1.0    False  \n",
      "3  This product does not work well at all.  We bo...        NaN    False  \n",
      "4  I have a cup of peppermint tea most mornings a...        1.0    False  \n",
      "0.072673992674\n"
     ]
    }
   ],
   "source": [
    "print(amazon.head())\n",
    "print(amazon['helpful'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 5e-06,
     "end_time": "2018-04-11T12:20:16.167011",
     "exception": false,
     "start_time": "2018-04-11T12:20:16.167006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature extraction on natural language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.009327,
     "end_time": "2018-04-11T12:20:16.185949",
     "exception": false,
     "start_time": "2018-04-11T12:20:16.176622",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# corpus = amazon.Text.as_matrix()\n",
    "# X_bag_of_words = vectorizer.fit_transform(corpus)\n",
    "# print(X_bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.259221,
     "end_time": "2018-04-11T12:20:16.446596",
     "exception": false,
     "start_time": "2018-04-11T12:20:16.187375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('popular')\n",
    "\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 116.362017,
     "end_time": "2018-04-11T12:22:12.808703",
     "exception": false,
     "start_time": "2018-04-11T12:20:16.446686",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolfm2/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolfm2/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolfm2/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136500, 1048576)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEICAYAAACqMQjAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fef7d264e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vectorize Bag of Words from review text; as sparse matrix\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# amazon['reviewLen'] = amazon['Text'].str.len() # Use this later /X as gross dummy\n",
    "\n",
    "amazon['textLower'] = amazon['Text'].str.lower()\n",
    "# look at the text strip_accents=ascii,  stop_words={'english'}, token_pattern = r'\\b[a-zA-Z0-9]{3,}\\b'),\n",
    "if isTraining:\n",
    "    hv0 = HashingVectorizer(n_features=2 ** 19, non_negative=True, tokenizer=LemmaTokenizer(), ngram_range=(1,3))\n",
    "    X_hv0 = hv0.fit_transform(amazon.textLower)\n",
    "else:\n",
    "    hv0 = joblib.load('hv0.pkl') # pickle\n",
    "    X_hv0 = hv0.transform(amazon.textLower)\n",
    "\n",
    "\n",
    "amazon['summaryFilter'] = amazon['Summary'].apply(lambda x: \" \" if x is np.nan else x) # some were np.nans\n",
    "amazon['sfLower'] = amazon['summaryFilter'].str.lower()\n",
    "# # and a second domain where we look at the summary\n",
    "if isTraining:\n",
    "    hv1 = HashingVectorizer(n_features=2 ** 19, non_negative=True, tokenizer=LemmaTokenizer(), ngram_range=(1,3))\n",
    "    X_hv1 = hv1.fit_transform(amazon.sfLower) \n",
    "else:\n",
    "    hv1 = joblib.load('hv1.pkl') # pickle\n",
    "    X_hv1 = hv1.transform(amazon.sfLower) \n",
    "\n",
    "# Another hash domain we want to count but not scale\n",
    "# amazon['timeFilter'] = amazon['Time'].apply(lambda x: str(int(x)%(86400 * 7))) # converts to day of week\n",
    "# hv2 = HashingVectorizer(n_features=2 ** 17, non_negative=True, strip_accents=ascii, \n",
    "#                            ngram_range=(1,1)) \n",
    "# X_hv2 = hv2.fit_transform(amazon.timeFilter + \" \" + amazon.ProductId + \" \" + amazon.UserId) # mw adds uid as token\n",
    "\n",
    "amazon['logReviewLen'] = np.round(np.log(amazon['Text'].str.len()),decimals=1) + 10\n",
    "amazon.hist(column=\"logReviewLen\")\n",
    "\n",
    "amazon['ScoreX'] = amazon['Score'].apply(lambda x: str(x)) # make score acceptable\n",
    "amazon['sLogReviewLen'] = amazon['logReviewLen'].apply(lambda x: str(x)) # make score acceptable\n",
    "if isTraining:\n",
    "    hv2 = HashingVectorizer(n_features=2 ** 17, non_negative=True, ngram_range=(1,1)) \n",
    "    X_hv2 = hv2.fit_transform(amazon.ScoreX + \" \" + amazon.UserId + \" \" + amazon.sLogReviewLen) # mw adds uid as token\n",
    "else:\n",
    "    hv2 = joblib.load('hv2.pkl') # pickle\n",
    "    X_hv2 = hv2.transform(amazon.ScoreX + \" \" + amazon.UserId + \" \" + amazon.sLogReviewLen) # mw adds uid as token\n",
    "\n",
    "import scipy.sparse as sp\n",
    "X_hv = sp.hstack([X_hv0, X_hv1], format='csr')\n",
    "print(X_hv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.008811,
     "end_time": "2018-04-11T12:22:12.817603",
     "exception": false,
     "start_time": "2018-04-11T12:22:12.808792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x = amazon.UserId + \" \" +  amazon.Text\n",
    "# x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.011196,
     "end_time": "2018-04-11T12:22:12.831679",
     "exception": false,
     "start_time": "2018-04-11T12:22:12.820483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want to be able to use this model fit on other data (the test set)\n",
    "# So let's save a copy of this instance of HashingVectorizer to be able to transform other data with this fit\n",
    "# http://scikit-learn.org/stable/modules/model_persistence.html\n",
    "if isTraining:\n",
    "    joblib.dump(hv0, 'hv0.pkl') # pickle\n",
    "    joblib.dump(hv1, 'hv1.pkl') # pickle\n",
    "    joblib.dump(hv2, 'hv2.pkl') # pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 4.315691,
     "end_time": "2018-04-11T12:22:17.147555",
     "exception": false,
     "start_time": "2018-04-11T12:22:12.831864",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "if isTraining:\n",
    "    transformer = TfidfTransformer()\n",
    "    X_tfidf = transformer.fit_transform(X_hv)\n",
    "    joblib.dump(transformer, 'transformer.pkl') # pickle\n",
    "else:\n",
    "    transformer = joblib.load('transformer.pkl') # pickle\n",
    "    X_tfidf = transformer.transform(X_hv)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.009499,
     "end_time": "2018-04-11T12:22:17.157145",
     "exception": false,
     "start_time": "2018-04-11T12:22:17.147646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 7e-06,
     "end_time": "2018-04-11T12:22:17.159926",
     "exception": false,
     "start_time": "2018-04-11T12:22:17.159919",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create additional quantitative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 4.808955,
     "end_time": "2018-04-11T12:22:21.978979",
     "exception": false,
     "start_time": "2018-04-11T12:22:17.170024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features from Amazon.csv to add to feature set\n",
    "import re\n",
    "\n",
    "#amazon['reviewLen'] = amazon['Text'].str.len()\n",
    "#amazon['summaryLen'] = amazon['summaryFilter'].str.len()\n",
    "\n",
    "#amazon['rlMeanDist'] = amazon['reviewLen'].apply(lambda x: abs(x-80)) # 80 is avg summary len. Thx George!\n",
    "#amazon['slMeanDist'] = amazon['summaryLen'].apply(lambda x: abs(x-8)) # 8. just guessing here.\n",
    "\n",
    "#import zlib\n",
    "#amazon['nameHash'] = zlib.crc32(str(amazon['UserId']).encode('utf8'))\n",
    "#amazon['nameHash'] = amazon['UserId'].apply(lambda x: zlib.crc32(str(x).encode('utf8'))) # bad. don't do it this way\n",
    "\n",
    "# stackoverflow.com/questions/15772371/finding-average-length-of-items-in-a-list-python\n",
    "# averages array element lengths\n",
    "def avgLen(text, regex):\n",
    "    lst = re.findall(regex, text)\n",
    "    lengths = [len(i) for i in lst]\n",
    "    return 0 if len(lengths) == 0 else (float(sum(lengths)) / len(lengths)) \n",
    "\n",
    "# ratio of regex to whole\n",
    "def cRatio(text, regex):\n",
    "    num = len(re.findall(regex, text))\n",
    "    text = \"\" if text is np.nan else text\n",
    "    den = len(text)\n",
    "    return 0 if den == 0 else num / den\n",
    "\n",
    "# Review Len\n",
    "amazon['summaryLen'] = amazon['Text'].str.len()\n",
    "\n",
    "# Num Words\n",
    "amazon['numWords'] = amazon['Text'].apply(lambda x: len(re.findall(\"[a-zA-Z']+\", x)))\n",
    "\n",
    "# Num Cap Words\n",
    "amazon['numCapWords'] = amazon['Text'].apply(lambda x: len(re.findall(\"[A-Z']+\", x)))\n",
    "\n",
    "# Avg Sentence Len\n",
    "amazon['avgSenLen'] = amazon['Text'].apply(lambda x: avgLen(x, \"[a-zA-Z' ]+\"))\n",
    "\n",
    "# Avg Word Len\n",
    "amazon['avgWrdLen'] = amazon['Text'].apply(lambda x: avgLen(x, \"[a-zA-Z']+\"))\n",
    "\n",
    "# ! Ratio\n",
    "amazon['ratioBang'] = amazon['Text'].apply(lambda x: cRatio(x, \"\\!\"))\n",
    "\n",
    "# ? Ratio                             \n",
    "amazon['ratioQmark'] = amazon['Text'].apply(lambda x: cRatio(x, \"\\?\"))\n",
    "\n",
    "# X_quant_features = amazon[[\"Score\", \"reviewLen\", \"summaryLen\", \"rlMeanDist\", \"slMeanDist\"]]\n",
    "# print(X_quant_features.head(10))\n",
    "# print(type(X_quant_features))\n",
    "X_quant_features = amazon[['summaryLen', 'numWords', 'numCapWords', 'avgSenLen', 'avgWrdLen', 'ratioBang', 'ratioQmark']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 6e-06,
     "end_time": "2018-04-11T12:22:21.979076",
     "exception": false,
     "start_time": "2018-04-11T12:22:21.979070",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Combine all quantitative features into a single sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 6.549336,
     "end_time": "2018-04-11T12:22:28.538634",
     "exception": false,
     "start_time": "2018-04-11T12:22:21.989298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136500, 1179655)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "X_quant_features_csr = csr_matrix(X_quant_features)\n",
    "X_combined = hstack([X_tfidf, X_quant_features_csr, X_hv2])  # we dont want to penalize hv2 w tfidf MW\n",
    "X_matrix = csr_matrix(X_combined) # convert to sparse matrix\n",
    "print(X_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 6e-06,
     "end_time": "2018-04-11T12:22:28.538792",
     "exception": false,
     "start_time": "2018-04-11T12:22:28.538786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create `X`, scaled matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.391977,
     "end_time": "2018-04-11T12:22:28.941762",
     "exception": false,
     "start_time": "2018-04-11T12:22:28.549785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136500, 1179655)\n"
     ]
    }
   ],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "if isTraining:\n",
    "    sc = StandardScaler(with_mean=False)\n",
    "    X = sc.fit_transform(X_matrix)\n",
    "    joblib.dump(sc, 'sc.pkl') # pickle\n",
    "else:\n",
    "    sc = joblib.load('sc.pkl')\n",
    "    X = sc.transform(X_matrix)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 6e-06,
     "end_time": "2018-04-11T12:22:28.941860",
     "exception": false,
     "start_time": "2018-04-11T12:22:28.941854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### create `y`, vector of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.012093,
     "end_time": "2018-04-11T12:22:28.964367",
     "exception": false,
     "start_time": "2018-04-11T12:22:28.952274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "y = amazon['helpful'].values\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 6e-06,
     "end_time": "2018-04-11T12:22:28.964521",
     "exception": false,
     "start_time": "2018-04-11T12:22:28.964515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.009295,
     "end_time": "2018-04-11T12:22:28.984456",
     "exception": false,
     "start_time": "2018-04-11T12:22:28.975161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from my_measures import BinaryClassificationPerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "papermill": {
     "duration": 0.07061,
     "end_time": "2018-04-11T12:22:29.058037",
     "exception": true,
     "start_time": "2018-04-11T12:22:28.987427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-17c1a959f9de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misTraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best.svm.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0msvm_performance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBinaryClassificationPerformance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'svm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0msvm_performance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_measures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_performance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperformance_measures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svm' is not defined"
     ]
    }
   ],
   "source": [
    "# # MODEL: SVM, linear\n",
    "# from sklearn import linear_model\n",
    "# svm = linear_model.SGDClassifier()\n",
    "# svm.fit(X, y)\n",
    "\n",
    "# joblib.dump(svm, 'svm.pkl') # pickle\n",
    "\n",
    "if not isTraining:\n",
    "    joblib.load('best.svm.pkl')\n",
    "    svm_performance = BinaryClassificationPerformance(svm.predict(X), y, 'svm')\n",
    "    svm_performance.compute_measures()\n",
    "    print(svm_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: logistic regression\n",
    "# from sklearn import linear_model\n",
    "# #lgs = linear_model.SGDClassifier(loss='log', n_iter=50, alpha=0.00001)\n",
    "# lgs = linear_model.SGDClassifier(loss='log', n_iter=1000, alpha=0.1)\n",
    "\n",
    "# lgs.fit(X, y)\n",
    "# joblib.dump(lgs, 'lgs.pkl') # pickle\n",
    "if not isTraining:\n",
    "    joblib.load('best.lgs.pkl')\n",
    "    lgs_performance = BinaryClassificationPerformance(lgs.predict(X), y, 'lgs')\n",
    "    lgs_performance.compute_measures()\n",
    "    print(lgs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: Naive Bayes\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# nbs = MultinomialNB()\n",
    "# nbs.fit(X, y)\n",
    "# joblib.dump(nbs, 'nbs.pkl') # pickle\n",
    "if not isTraining:\n",
    "    joblib.load('best.nbs.pkl')\n",
    "    nbs_performance = BinaryClassificationPerformance(nbs.predict(X), y, 'nbs')\n",
    "    nbs_performance.compute_measures()\n",
    "    print(nbs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: Ridge Regression Classifier\n",
    "# from sklearn import linear_model\n",
    "# rdg = linear_model.RidgeClassifier()\n",
    "# rdg.fit(X, y)\n",
    "# joblib.dump(rdg, 'rdg.pkl') # pickle\n",
    "if not isTraining:\n",
    "    joblib.load('best.rdg.pkl')\n",
    "    rdg_performance = BinaryClassificationPerformance(rdg.predict(X), y, 'rdg')\n",
    "    rdg_performance.compute_measures()\n",
    "    print(rdg_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: Perceptron\n",
    "# from sklearn import linear_model\n",
    "# prc = linear_model.SGDClassifier(loss='perceptron')\n",
    "# prc.fit(X, y)\n",
    "# joblib.dump(prc, 'prc.pkl') # pickle\n",
    "if not isTraining:\n",
    "    joblib.load('best.prc.pkl')\n",
    "    prc_performance = BinaryClassificationPerformance(prc.predict(X), y, 'prc')\n",
    "    prc_performance.compute_measures()\n",
    "    print(prc_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier # mw\n",
    "\n",
    "# prepare a range of alpha values to test\n",
    "# alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "alphas = np.array([1, 0.1, 0.01, 0.001, 0.0001, 0.00001])\n",
    "Cs = np.array([0.001, 0.01, 0.1, 1, 10, 100, 1000])\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "# model = linear_model.SGDClassifier(loss='perceptron', max_iter=50) # max_iter 1000\n",
    "\n",
    "mlp = MLPClassifier(random_state=0)\n",
    "svm = linear_model.SGDClassifier(n_iter=500)\n",
    "lgs = linear_model.SGDClassifier(loss='log', n_iter=500)\n",
    "nbs = MultinomialNB()\n",
    "rdg = linear_model.RidgeClassifier()\n",
    "prc = linear_model.SGDClassifier(loss='perceptron', n_iter=500)\n",
    "\n",
    "if isTraining:\n",
    "    mList = [[svm,\"svm\"], [lgs,\"lgs\"], [prc,\"prc\"], [nbs,\"nbs\"], [rdg,\"rdg\"]]\n",
    "else:\n",
    "    mList = []\n",
    "\n",
    "for model in mList: \n",
    "# for model in []: \n",
    "# for model in [rdg]:    \n",
    "  fh = open(\"GridSearch.txt\", \"a\")\n",
    "  grid = GridSearchCV(estimator=model[0], param_grid=dict(alpha=alphas), n_jobs=2) #\n",
    "  grid.fit(X, y)\n",
    "  print(grid)\n",
    "  # summarize the results of the grid search\n",
    "  print(grid.cv_results_)\n",
    "  print(grid.best_score_)\n",
    "  print(grid.best_estimator_.alpha)\n",
    "\n",
    "  fh.write('\\n########\\n')\n",
    "  fh.write(str(datetime.datetime.now()))\n",
    "  fh.write('\\n########\\n')\n",
    "  fh.write(str(model[0]) + '\\n')  \n",
    "  fh.write(str(grid.cv_results_).replace(\", '\", \",\\n'\") + '\\n')\n",
    "  fh.write(str(grid.best_score_) + '\\n')  \n",
    "  fh.write(str(grid.best_estimator_.alpha) + '\\n')\n",
    "  fh.close()\n",
    "\n",
    "  # MODEL: BEST\n",
    "  best = grid.best_estimator_\n",
    "\n",
    "  best.fit(X, y)\n",
    "  joblib.dump(best, 'best.{}.pkl'.format(model[1])) # pickle\n",
    "\n",
    "  best_performance = BinaryClassificationPerformance(best.predict(X), y, 'best')\n",
    "  best_performance.compute_measures()\n",
    "  print(best_performance.performance_measures)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pg = {'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "'hidden_layer_sizes': [(100,1), (100,2), (100,3)],\n",
    "#'alpha': [10.0 ** -np.arange(1, 7)],\n",
    "'alpha': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "'activation': [\"logistic\", \"relu\", \"Tanh\"],\n",
    "'tol': [1e-2, 1e-4, 1e-6],\n",
    "'epsilon': [1e-3, 1e-7, 1e-8, 1e-9, 1e-8]\n",
    "}\n",
    "\n",
    "fh = open(\"GridSearch.txt\", \"a\")\n",
    "grid = GridSearchCV(estimator=mlp, param_grid=pg, n_jobs=2) #\n",
    "grid.fit(X, y)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.cv_results_)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)\n",
    "\n",
    "fh.write('\\n########\\n')\n",
    "fh.write(str(datetime.datetime.now()))\n",
    "fh.write('\\n########\\n')\n",
    "fh.write(str(model) + '\\n')  \n",
    "fh.write(str(grid.cv_results_).replace(\", '\", \",\\n'\") + '\\n')\n",
    "fh.write(str(grid.best_score_) + '\\n')  \n",
    "fh.write(str(grid.best_estimator_.alpha) + '\\n')\n",
    "fh.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# MODEL: BEST\n",
    "best = grid.best_estimator_\n",
    "\n",
    "best.fit(X, y)\n",
    "joblib.dump(best, 'best.pkl') # pickle\n",
    "\n",
    "best_performance = BinaryClassificationPerformance(best.predict(X), y, 'best')\n",
    "best_performance.compute_measures()\n",
    "print(best_performance.performance_measures)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### ROC plot to compare performance of various models and fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fits = [svm_performance, lgs_performance, nbs_performance, rdg_performance, prc_performance]\n",
    "# fits = [svm_performance, lgs_performance, rdg_performance, prc_performance]\n",
    "\n",
    "if isTraining:\n",
    "    fList = []\n",
    "else:\n",
    "    fList = fits\n",
    "\n",
    "for fit in fList:\n",
    "    plt.plot(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], 'ro')\n",
    "    plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], fit.desc)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.title('ROC plot: training set')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "papermill": {
   "duration": 135.669871,
   "end_time": "2018-04-11T12:22:30.046787",
   "environment_variables": {},
   "exception": true,
   "output_path": "output.ipynb",
   "parameters": null,
   "start_time": "2018-04-11T12:20:14.376916",
   "version": "0.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}