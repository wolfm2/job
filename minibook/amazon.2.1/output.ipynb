{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.000832,
     "end_time": "2018-04-15T21:59:06.113991",
     "exception": false,
     "start_time": "2018-04-15T21:59:06.113159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.471347,
     "end_time": "2018-04-15T21:59:06.601321",
     "exception": false,
     "start_time": "2018-04-15T21:59:06.129974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.041554,
     "end_time": "2018-04-15T21:59:06.642980",
     "exception": false,
     "start_time": "2018-04-15T21:59:06.601426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.system('ps aux | grep wolfm2')\n",
    "#os.system('killall -s SIGKILL -u wolfm2')\n",
    "#os.system('cp /home/wolfm2/job.sh .; echo test 1>&2') #; cp ../job.log ../jerbb.txt')\n",
    "\n",
    "isTraining = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1e-05,
     "end_time": "2018-04-15T21:59:06.643066",
     "exception": false,
     "start_time": "2018-04-15T21:59:06.643056",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Read raw training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 2.465857,
     "end_time": "2018-04-15T21:59:09.122279",
     "exception": false,
     "start_time": "2018-04-15T21:59:06.656422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364000, 14)\n"
     ]
    }
   ],
   "source": [
    "if isTraining:\n",
    "    #amazon = pd.read_csv('/home/wolfm2/amazon_data.0/raw_data_train.csv')\n",
    "    amazon = pd.read_csv('/home/wolfm2/amazon_data/raw_data_train.csv')\n",
    "    #amazon = pd.read_csv('/home/ich/amazon_data/raw_data_train.csv')\n",
    "else:\n",
    "    amazon = pd.read_csv('/home/wolfm2/amazon_data.0/raw_data_test.csv')\n",
    "\n",
    "print(amazon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.032183,
     "end_time": "2018-04-15T21:59:09.154637",
     "exception": false,
     "start_time": "2018-04-15T21:59:09.122454",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 0.1      Id   ProductId          UserId  \\\n",
      "0      150581        487850  487851  B0025UCD76  A28B2M0XRXHXIG   \n",
      "1      334018         21518   21519  B002QWP89S   A7JJX3KMDZD2F   \n",
      "2       76657        319457  319458  B001GVIUX6  A2S8RJ6DRKGYON   \n",
      "3      357903        248851  248852  B0009JRH1C  A1FLQ698D9C0C8   \n",
      "4      301824        394613  394614  B001B4VOQI  A2KJO9EPX17ZXE   \n",
      "\n",
      "                   ProfileName  HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
      "0                         B622                     0                       0   \n",
      "1  Shinichi Isozaki \"shincyan\"                     1                       2   \n",
      "2                   M. Ronning                     1                       2   \n",
      "3                     G. Zhang                     4                       8   \n",
      "4                    Musical E                     0                       0   \n",
      "\n",
      "   Score        Time                                            Summary  \\\n",
      "0      5  1313020800                                         DELICIOUS!   \n",
      "1      5  1268524800                     The pet dog is delighted, too!   \n",
      "2      2  1313798400  may be healthy but my \"eat anything\" cat won't...   \n",
      "3      5  1255478400                  Weight Loss Benefits of Green Tea   \n",
      "4      5  1305849600                     Healthy High Quality Dog Treat   \n",
      "\n",
      "                                                Text  helpScore  helpful  \n",
      "0  This BBQ sauce is DELICIOUS!!  Sweet and tangy...        NaN    False  \n",
      "1  I gave a pet dog plural resemblance products, ...        0.5    False  \n",
      "2  I tried this in place of Iams.  My hefty maine...        0.5    False  \n",
      "3  Weight Loss Benefits of Green Tea<br />=======...        0.5    False  \n",
      "4  Yes, they are a bit expensive but, they are hi...        NaN    False  \n",
      "0.073206043956\n"
     ]
    }
   ],
   "source": [
    "print(amazon.head())\n",
    "print(amazon['helpful'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.6e-05,
     "end_time": "2018-04-15T21:59:09.154929",
     "exception": false,
     "start_time": "2018-04-15T21:59:09.154913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature extraction on natural language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.018118,
     "end_time": "2018-04-15T21:59:09.201096",
     "exception": false,
     "start_time": "2018-04-15T21:59:09.182978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# corpus = amazon.Text.as_matrix()\n",
    "# X_bag_of_words = vectorizer.fit_transform(corpus)\n",
    "# print(X_bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.41594,
     "end_time": "2018-04-15T21:59:09.635373",
     "exception": false,
     "start_time": "2018-04-15T21:59:09.219433",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('popular')\n",
    "\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 307.550625,
     "end_time": "2018-04-15T22:04:17.186177",
     "exception": false,
     "start_time": "2018-04-15T21:59:09.635552",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolfm2/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/home/wolfm2/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolfm2/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/home/wolfm2/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364000, 524288)\n",
      "   Unnamed: 0  Unnamed: 0.1      Id   ProductId          UserId  \\\n",
      "0      150581        487850  487851  B0025UCD76  A28B2M0XRXHXIG   \n",
      "1      334018         21518   21519  B002QWP89S   A7JJX3KMDZD2F   \n",
      "2       76657        319457  319458  B001GVIUX6  A2S8RJ6DRKGYON   \n",
      "3      357903        248851  248852  B0009JRH1C  A1FLQ698D9C0C8   \n",
      "4      301824        394613  394614  B001B4VOQI  A2KJO9EPX17ZXE   \n",
      "\n",
      "                   ProfileName  HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
      "0                         B622                     0                       0   \n",
      "1  Shinichi Isozaki \"shincyan\"                     1                       2   \n",
      "2                   M. Ronning                     1                       2   \n",
      "3                     G. Zhang                     4                       8   \n",
      "4                    Musical E                     0                       0   \n",
      "\n",
      "   Score        Time                                            Summary  \\\n",
      "0      5  1313020800                                         DELICIOUS!   \n",
      "1      5  1268524800                     The pet dog is delighted, too!   \n",
      "2      2  1313798400  may be healthy but my \"eat anything\" cat won't...   \n",
      "3      5  1255478400                  Weight Loss Benefits of Green Tea   \n",
      "4      5  1305849600                     Healthy High Quality Dog Treat   \n",
      "\n",
      "                                                Text  helpScore  helpful  \\\n",
      "0  This BBQ sauce is DELICIOUS!!  Sweet and tangy...        NaN    False   \n",
      "1  I gave a pet dog plural resemblance products, ...        0.5    False   \n",
      "2  I tried this in place of Iams.  My hefty maine...        0.5    False   \n",
      "3  Weight Loss Benefits of Green Tea<br />=======...        0.5    False   \n",
      "4  Yes, they are a bit expensive but, they are hi...        NaN    False   \n",
      "\n",
      "  ScoreX  \n",
      "0      5  \n",
      "1      5  \n",
      "2      2  \n",
      "3      5  \n",
      "4      5  \n"
     ]
    }
   ],
   "source": [
    "# vectorize Bag of Words from review text; as sparse matrix\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# amazon['reviewLen'] = amazon['Text'].str.len() # Use this later /X as gross dummy\n",
    "\n",
    "# amazon['textLower'] = amazon['Text'].str.lower()\n",
    "# look at the text  token_pattern = r'\\b[a-zA-Z0-9]{3,}\\b',\n",
    "if isTraining:\n",
    "    hv0 = HashingVectorizer(n_features=2 ** 19, non_negative=True, tokenizer=LemmaTokenizer(), strip_accents=ascii,  stop_words={'english'}, token_pattern = r'\\b[a-zA-Z]{3,}\\b', ngram_range=(1,2))\n",
    "    X_hv0 = hv0.fit_transform(amazon.Text)\n",
    "    joblib.dump(hv0, 'hv0.pkl') # pickle\n",
    "else:\n",
    "    hv0 = joblib.load('hv0.pkl') # pickle\n",
    "    X_hv0 = hv0.transform(amazon.Text)\n",
    "\n",
    "\n",
    "# amazon['summaryFilter'] = amazon['Summary'].apply(lambda x: \" \" if x is np.nan else x) # some were np.nans\n",
    "# amazon['sfLower'] = amazon['summaryFilter'].str.lower()\n",
    "# # and a second domain where we look at the summary\n",
    "# if isTraining:\n",
    "#     hv1 = HashingVectorizer(n_features=2 ** 19, non_negative=True, tokenizer=LemmaTokenizer(), ngram_range=(1,3))\n",
    "#     X_hv1 = hv1.fit_transform(amazon.sfLower) \n",
    "#     joblib.dump(hv1, 'hv1.pkl') # pickle\n",
    "# else:\n",
    "#     hv1 = joblib.load('hv1.pkl') # pickle\n",
    "#     X_hv1 = hv1.transform(amazon.sfLower) \n",
    "\n",
    "# Another hash domain we want to count but not tfidf\n",
    "# amazon['timeFilter'] = amazon['Time'].apply(lambda x: str(int(x)%(86400 * 7))) # converts to day of week\n",
    "# hv2 = HashingVectorizer(n_features=2 ** 17, non_negative=True, strip_accents=ascii, \n",
    "#                            ngram_range=(1,1)) \n",
    "# X_hv2 = hv2.fit_transform(amazon.timeFilter + \" \" + amazon.ProductId + \" \" + amazon.UserId) # mw adds uid as token\n",
    "\n",
    "# amazon['logReviewLen'] = np.round(np.log(amazon['Text'].str.len()),decimals=1) + 10\n",
    "# amazon.hist(column=\"logReviewLen\")\n",
    "\n",
    "amazon['ScoreX'] = amazon['Score'].apply(lambda x: str(x)) # make score acceptable\n",
    "# amazon['sLogReviewLen'] = amazon['logReviewLen'].apply(lambda x: str(x)) # make score acceptable\n",
    "if isTraining:\n",
    "    hv2 = HashingVectorizer(n_features=2 ** 17, non_negative=True, ngram_range=(1,1)) \n",
    "    X_hv2 = hv2.fit_transform(amazon.ScoreX) # mw adds uid as token\n",
    "    joblib.dump(hv2, 'hv2.pkl') # pickle\n",
    "else:\n",
    "    hv2 = joblib.load('hv2.pkl') # pickle\n",
    "    X_hv2 = hv2.transform(amazon.ScoreX) # mw adds uid as token\n",
    "\n",
    "import scipy.sparse as sp\n",
    "# X_hv = sp.hstack([X_hv0], format='csr')\n",
    "X_hv = X_hv0\n",
    "print(X_hv.shape)\n",
    "\n",
    "print(amazon.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.014329,
     "end_time": "2018-04-15T22:04:17.200617",
     "exception": false,
     "start_time": "2018-04-15T22:04:17.186288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x = amazon.UserId + \" \" +  amazon.Text\n",
    "# x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.014235,
     "end_time": "2018-04-15T22:04:17.215119",
     "exception": false,
     "start_time": "2018-04-15T22:04:17.200884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We want to be able to use this model fit on other data (the test set)\n",
    "# So let's save a copy of this instance of HashingVectorizer to be able to transform other data with this fit\n",
    "# http://scikit-learn.org/stable/modules/model_persistence.html\n",
    "# if isTraining:\n",
    "#     joblib.dump(hv0, 'hv0.pkl') # pickle\n",
    "#     joblib.dump(hv1, 'hv1.pkl') # pickle\n",
    "#     joblib.dump(hv2, 'hv2.pkl') # pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 7.77218,
     "end_time": "2018-04-15T22:04:24.987343",
     "exception": false,
     "start_time": "2018-04-15T22:04:17.215163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "if isTraining:\n",
    "    transformer = TfidfTransformer()\n",
    "    X_tfidf = transformer.fit_transform(X_hv, X_hv2)\n",
    "    joblib.dump(transformer, 'transformer.pkl') # pickle\n",
    "else:\n",
    "    transformer = joblib.load('transformer.pkl') # pickle\n",
    "    X_tfidf = transformer.transform(X_hv)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.009919,
     "end_time": "2018-04-15T22:04:24.997362",
     "exception": false,
     "start_time": "2018-04-15T22:04:24.987443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 1e-05,
     "end_time": "2018-04-15T22:04:25.002412",
     "exception": false,
     "start_time": "2018-04-15T22:04:25.002402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create additional quantitative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 12.884306,
     "end_time": "2018-04-15T22:04:37.899306",
     "exception": false,
     "start_time": "2018-04-15T22:04:25.015000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 0.1      Id   ProductId          UserId  \\\n",
      "0      150581        487850  487851  B0025UCD76  A28B2M0XRXHXIG   \n",
      "1      334018         21518   21519  B002QWP89S   A7JJX3KMDZD2F   \n",
      "2       76657        319457  319458  B001GVIUX6  A2S8RJ6DRKGYON   \n",
      "3      357903        248851  248852  B0009JRH1C  A1FLQ698D9C0C8   \n",
      "4      301824        394613  394614  B001B4VOQI  A2KJO9EPX17ZXE   \n",
      "\n",
      "                   ProfileName  HelpfulnessNumerator  HelpfulnessDenominator  \\\n",
      "0                         B622                     0                       0   \n",
      "1  Shinichi Isozaki \"shincyan\"                     1                       2   \n",
      "2                   M. Ronning                     1                       2   \n",
      "3                     G. Zhang                     4                       8   \n",
      "4                    Musical E                     0                       0   \n",
      "\n",
      "   Score        Time                                            Summary  \\\n",
      "0      5  1313020800                                         DELICIOUS!   \n",
      "1      5  1268524800                     The pet dog is delighted, too!   \n",
      "2      2  1313798400  may be healthy but my \"eat anything\" cat won't...   \n",
      "3      5  1255478400                  Weight Loss Benefits of Green Tea   \n",
      "4      5  1305849600                     Healthy High Quality Dog Treat   \n",
      "\n",
      "                                                Text  helpScore  helpful  \\\n",
      "0  This BBQ sauce is DELICIOUS!!  Sweet and tangy...        NaN    False   \n",
      "1  I gave a pet dog plural resemblance products, ...        0.5    False   \n",
      "2  I tried this in place of Iams.  My hefty maine...        0.5    False   \n",
      "3  Weight Loss Benefits of Green Tea<br />=======...        0.5    False   \n",
      "4  Yes, they are a bit expensive but, they are hi...        NaN    False   \n",
      "\n",
      "  ScoreX  summaryLen  numWords  numCapWords  avgSenLen  avgWrdLen  \n",
      "0      5         110        20            5  33.333333   3.900000  \n",
      "1      5         140        25            3  26.600000   4.400000  \n",
      "2      2         471        96           11  46.000000   3.729167  \n",
      "3      5       10800      1788          197  33.523026   4.729306  \n",
      "4      5         152        26            3  24.333333   4.576923  \n"
     ]
    }
   ],
   "source": [
    "# features from Amazon.csv to add to feature set\n",
    "import re\n",
    "\n",
    "#amazon['reviewLen'] = amazon['Text'].str.len()\n",
    "#amazon['summaryLen'] = amazon['summaryFilter'].str.len()\n",
    "\n",
    "#amazon['rlMeanDist'] = amazon['reviewLen'].apply(lambda x: abs(x-80)) # 80 is avg summary len. Thx George!\n",
    "#amazon['slMeanDist'] = amazon['summaryLen'].apply(lambda x: abs(x-8)) # 8. just guessing here.\n",
    "\n",
    "#import zlib\n",
    "#amazon['nameHash'] = zlib.crc32(str(amazon['UserId']).encode('utf8'))\n",
    "#amazon['nameHash'] = amazon['UserId'].apply(lambda x: zlib.crc32(str(x).encode('utf8'))) # bad. don't do it this way\n",
    "\n",
    "# stackoverflow.com/questions/15772371/finding-average-length-of-items-in-a-list-python\n",
    "# averages array element lengths\n",
    "def avgLen(text, regex):\n",
    "    lst = re.findall(regex, text)\n",
    "    lengths = [len(i) for i in lst]\n",
    "    return 0 if len(lengths) == 0 else (float(sum(lengths)) / len(lengths)) \n",
    "\n",
    "# ratio of regex to whole\n",
    "def cRatio(text, regex):\n",
    "    num = len(re.findall(regex, text))\n",
    "    text = \"\" if text is np.nan else text\n",
    "    den = len(text)\n",
    "    return 0 if den == 0 else num / den\n",
    "\n",
    "# Review Len\n",
    "amazon['summaryLen'] = amazon['Text'].str.len()\n",
    "\n",
    "# Num Words\n",
    "amazon['numWords'] = amazon['Text'].apply(lambda x: len(re.findall(\"[a-zA-Z']+\", x)))\n",
    "\n",
    "# Num Cap Words\n",
    "amazon['numCapWords'] = amazon['Text'].apply(lambda x: len(re.findall(\"[A-Z']+\", x)))\n",
    "\n",
    "# Avg Sentence Len\n",
    "amazon['avgSenLen'] = amazon['Text'].apply(lambda x: avgLen(x, \"[a-zA-Z' ]+\"))\n",
    "\n",
    "# Avg Word Len\n",
    "amazon['avgWrdLen'] = amazon['Text'].apply(lambda x: avgLen(x, \"[a-zA-Z']+\"))\n",
    "\n",
    "# ! Ratio\n",
    "# amazon['ratioBang'] = amazon['Text'].apply(lambda x: cRatio(x, \"\\!\"))\n",
    "\n",
    "# ? Ratio                             \n",
    "# amazon['ratioQmark'] = amazon['Text'].apply(lambda x: cRatio(x, \"\\?\"))\n",
    "\n",
    "print(amazon.head())\n",
    "\n",
    "# X_quant_features = amazon[[\"Score\", \"reviewLen\", \"summaryLen\", \"rlMeanDist\", \"slMeanDist\"]]\n",
    "# print(X_quant_features.head(10))\n",
    "# print(type(X_quant_features))\n",
    "X_quant_features = amazon[['summaryLen', 'numWords', 'numCapWords', 'avgSenLen', 'avgWrdLen', 'Score']]\n",
    "# X_quant_features = amazon[[]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 8e-06,
     "end_time": "2018-04-15T22:04:37.899416",
     "exception": false,
     "start_time": "2018-04-15T22:04:37.899408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Combine all quantitative features into a single sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 12.576216,
     "end_time": "2018-04-15T22:04:50.488383",
     "exception": false,
     "start_time": "2018-04-15T22:04:37.912167",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364000, 524294)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "X_quant_features_csr = csr_matrix(X_quant_features)\n",
    "X_combined = hstack([X_tfidf, X_quant_features_csr])  # we dont want to penalize hv2 w tfidf MW\n",
    "X_matrix = csr_matrix(X_combined) # convert to sparse matrix\n",
    "print(X_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 7e-06,
     "end_time": "2018-04-15T22:04:50.488498",
     "exception": false,
     "start_time": "2018-04-15T22:04:50.488491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create `X`, scaled matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 1.7265,
     "end_time": "2018-04-15T22:04:52.228264",
     "exception": false,
     "start_time": "2018-04-15T22:04:50.501764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364000, 524294)\n"
     ]
    }
   ],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "if isTraining:\n",
    "    sc = StandardScaler(with_mean=False)\n",
    "    X = sc.fit_transform(X_matrix)\n",
    "    joblib.dump(sc, 'sc.pkl') # pickle\n",
    "else:\n",
    "    sc = joblib.load('sc.pkl')\n",
    "    X = sc.transform(X_matrix)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.1e-05,
     "end_time": "2018-04-15T22:04:52.228386",
     "exception": false,
     "start_time": "2018-04-15T22:04:52.228375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### create `y`, vector of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.018222,
     "end_time": "2018-04-15T22:04:52.259875",
     "exception": false,
     "start_time": "2018-04-15T22:04:52.241653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "y = amazon['helpful'].values\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 9e-06,
     "end_time": "2018-04-15T22:04:52.259938",
     "exception": false,
     "start_time": "2018-04-15T22:04:52.259929",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.018191,
     "end_time": "2018-04-15T22:04:52.291541",
     "exception": false,
     "start_time": "2018-04-15T22:04:52.273350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from my_measures import BinaryClassificationPerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 129.06363,
     "end_time": "2018-04-15T22:07:01.355234",
     "exception": false,
     "start_time": "2018-04-15T22:04:52.291604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "if isTraining:\n",
    "    m0 = LogisticRegression()\n",
    "    m0.fit(X, y)\n",
    "    joblib.dump(m0, 'log.pkl')\n",
    "else:\n",
    "    m0 = joblib.load('log.pkl')\n",
    "    m0 = BinaryClassificationPerformance(m0.predict(X), y, 'log')\n",
    "    m0_performance.compute_measures()\n",
    "    print(m0_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 5503.765592,
     "end_time": "2018-04-15T23:38:45.120986",
     "exception": false,
     "start_time": "2018-04-15T22:07:01.355394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "if isTraining:\n",
    "    m0 = tree.DecisionTreeClassifier(criterion='gini')\n",
    "    m0.fit(X, y)\n",
    "    joblib.dump(m0, 'tree.pkl')\n",
    "else:\n",
    "    m0 = joblib.load('tree.pkl')\n",
    "    m1_performance = BinaryClassificationPerformance(m0.predict(X), y, 'tree')\n",
    "    m1_performance.compute_measures()\n",
    "    print(m1_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.010801,
     "end_time": "2018-04-15T23:38:45.131886",
     "exception": false,
     "start_time": "2018-04-15T23:38:45.121085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn import svm\\n\\nif isTraining:\\n    m0 = svm.svc()\\n    m0.fit(X, y)\\n    joblib.dump(m0, 'svm.pkl')\\nelse:\\n    m0 = joblib.load('svm.pkl')\\n    m2_performance = BinaryClassificationPerformance(m0.predict(X), y, 'svm')\\n    m2_performance.compute_measures()\\n    print(m2_performance.performance_measures)\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn import svm\n",
    "\n",
    "if isTraining:\n",
    "    m0 = svm.svc()\n",
    "    m0.fit(X, y)\n",
    "    joblib.dump(m0, 'svm.pkl')\n",
    "else:\n",
    "    m0 = joblib.load('svm.pkl')\n",
    "    m2_performance = BinaryClassificationPerformance(m0.predict(X), y, 'svm')\n",
    "    m2_performance.compute_measures()\n",
    "    print(m2_performance.performance_measures)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.01002,
     "end_time": "2018-04-15T23:38:45.146495",
     "exception": false,
     "start_time": "2018-04-15T23:38:45.136475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.naive_bayes import GaussianNB\\n\\nif isTraining:\\n    m0 = GaussianNB()\\n    m0.fit(X, y)\\n    joblib.dump(m0, 'nb.pkl')\\nelse:\\n    m0 = joblib.load('nb.pkl')\\n    m3_performance = BinaryClassificationPerformance(m0.predict(X), y, 'nb')\\n    m3_performance.compute_measures()\\n    print(m3_performance.performance_measures)\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "if isTraining:\n",
    "    m0 = GaussianNB()\n",
    "    m0.fit(X, y)\n",
    "    joblib.dump(m0, 'nb.pkl')\n",
    "else:\n",
    "    m0 = joblib.load('nb.pkl')\n",
    "    m3_performance = BinaryClassificationPerformance(m0.predict(X), y, 'nb')\n",
    "    m3_performance.compute_measures()\n",
    "    print(m3_performance.performance_measures)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.378852,
     "end_time": "2018-04-15T23:38:45.530901",
     "exception": false,
     "start_time": "2018-04-15T23:38:45.152049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "if isTraining:\n",
    "    m0 = KNeighborsClassifier(n_neighbors=6)\n",
    "    m0.fit(X, y)\n",
    "    joblib.dump(m0, 'kn.pkl')\n",
    "else:\n",
    "    m0 = joblib.load('kn.pkl')\n",
    "    m4_performance = BinaryClassificationPerformance(m0.predict(X), y, 'kn')\n",
    "    m4_performance.compute_measures()\n",
    "    print(m4_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "if isTraining:\n",
    "    m0 = RandomForestClassifier()\n",
    "    m0.fit(X, y)\n",
    "    joblib.dump(m0, 'rf.pkl')\n",
    "else:\n",
    "    m0 = joblib.load('rf.pkl')\n",
    "    m5_performance = BinaryClassificationPerformance(m0.predict(X), y, 'rf')\n",
    "    m5_performance.compute_measures()\n",
    "    print(m5_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "if isTraining:\n",
    "    m0 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "    m0.fit(X, y)\n",
    "    joblib.dump(m0, 'gb.pkl')\n",
    "else:\n",
    "    m0 = joblib.load('gb.pkl')\n",
    "    m6_performance = BinaryClassificationPerformance(m0.predict(X), y, 'gb')\n",
    "    m6_performance.compute_measures()\n",
    "    print(m6_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "if isTraining:\n",
    "    m0 = XGBClassifier()\n",
    "    m0.fit(X, y)\n",
    "    joblib.dump(m0, 'xgb.pkl')\n",
    "else:\n",
    "    m0 = joblib.load('xgb.pkl')\n",
    "    m7_performance = BinaryClassificationPerformance(m0.predict(X), y, 'xgb')\n",
    "    m7_performance.compute_measures()\n",
    "    print(m7_performance.performance_measures)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''from catboost import CatBoostRegressor\n",
    "\n",
    "if isTraining:\n",
    "    m0 = CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')\n",
    "    m0.fit(X, y)\n",
    "    joblib.dump(m0, 'cb.pkl')\n",
    "else:\n",
    "    m0 = joblib.load('cb.pkl')\n",
    "    m8_performance = BinaryClassificationPerformance(m0.predict(X), y, 'cb')\n",
    "    m8_performance.compute_measures()\n",
    "    print(m8_performance.performance_measures)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: SVM, linear\n",
    "# from sklearn import linear_model\n",
    "# svm = linear_model.SGDClassifier()\n",
    "# svm.fit(X, y)\n",
    "\n",
    "# joblib.dump(svm, 'svm.pkl') # pickle\n",
    "\n",
    "if not isTraining:\n",
    "    svm = joblib.load('best.svm.pkl')\n",
    "    svm_performance = BinaryClassificationPerformance(svm.predict(X), y, 'svm')\n",
    "    svm_performance.compute_measures()\n",
    "    print(svm_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: logistic regression\n",
    "# from sklearn import linear_model\n",
    "# #lgs = linear_model.SGDClassifier(loss='log', n_iter=50, alpha=0.00001)\n",
    "# lgs = linear_model.SGDClassifier(loss='log', n_iter=1000, alpha=0.1)\n",
    "\n",
    "# lgs.fit(X, y)\n",
    "# joblib.dump(lgs, 'lgs.pkl') # pickle\n",
    "if not isTraining:\n",
    "    lgs = joblib.load('best.lgs.pkl')\n",
    "    lgs_performance = BinaryClassificationPerformance(lgs.predict(X), y, 'lgs')\n",
    "    lgs_performance.compute_measures()\n",
    "    print(lgs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: Naive Bayes\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# nbs = MultinomialNB()\n",
    "# nbs.fit(X, y)\n",
    "# joblib.dump(nbs, 'nbs.pkl') # pickle\n",
    "if not isTraining:\n",
    "    nbs = joblib.load('best.nbs.pkl')\n",
    "    nbs_performance = BinaryClassificationPerformance(nbs.predict(X), y, 'nbs')\n",
    "    nbs_performance.compute_measures()\n",
    "    print(nbs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: Ridge Regression Classifier\n",
    "# from sklearn import linear_model\n",
    "# rdg = linear_model.RidgeClassifier()\n",
    "# rdg.fit(X, y)\n",
    "# joblib.dump(rdg, 'rdg.pkl') # pickle\n",
    "if not isTraining:\n",
    "    rdg = joblib.load('best.rdg.pkl')\n",
    "    rdg_performance = BinaryClassificationPerformance(rdg.predict(X), y, 'rdg')\n",
    "    rdg_performance.compute_measures()\n",
    "    print(rdg_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: Perceptron\n",
    "# from sklearn import linear_model\n",
    "# prc = linear_model.SGDClassifier(loss='perceptron')\n",
    "# prc.fit(X, y)\n",
    "# joblib.dump(prc, 'prc.pkl') # pickle\n",
    "if not isTraining:\n",
    "    prc = joblib.load('best.prc.pkl')\n",
    "    prc_performance = BinaryClassificationPerformance(prc.predict(X), y, 'prc')\n",
    "    prc_performance.compute_measures()\n",
    "    print(prc_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier # mw\n",
    "\n",
    "# prepare a range of alpha values to test\n",
    "# alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "alphas = np.array([1, 0.1, 0.01, 0.001, 0.0001, 0.00001])\n",
    "Cs = np.array([0.001, 0.01, 0.1, 1, 10, 100, 1000])\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "# model = linear_model.SGDClassifier(loss='perceptron', max_iter=50) # max_iter 1000\n",
    "\n",
    "if isTraining:\n",
    "    mlp = MLPClassifier(random_state=0)\n",
    "    svm = linear_model.SGDClassifier(n_iter=500)\n",
    "    lgs = linear_model.SGDClassifier(loss='log', n_iter=500)\n",
    "    nbs = MultinomialNB()\n",
    "    rdg = linear_model.RidgeClassifier()\n",
    "    prc = linear_model.SGDClassifier(loss='perceptron', n_iter=500)\n",
    "    mList = [[prc,\"prc\"], [nbs,\"nbs\"], [rdg,\"rdg\"]] # [svm,\"svm\"], [lgs,\"lgs\"], \n",
    "    \n",
    "else:\n",
    "    mList = []\n",
    "\n",
    "for model in mList: \n",
    "# for model in []: \n",
    "# for model in [rdg]:    \n",
    "  fh = open(\"GridSearch.txt\", \"a\")\n",
    "  grid = GridSearchCV(estimator=model[0], param_grid=dict(alpha=alphas), n_jobs=2) #\n",
    "  grid.fit(X, y)\n",
    "  print(grid)\n",
    "  # summarize the results of the grid search\n",
    "  print(grid.cv_results_)\n",
    "  print(grid.best_score_)\n",
    "  print(grid.best_estimator_.alpha)\n",
    "\n",
    "  fh.write('\\n########\\n')\n",
    "  fh.write(str(datetime.datetime.now()))\n",
    "  fh.write('\\n########\\n')\n",
    "  fh.write(str(model[0]) + '\\n')  \n",
    "  fh.write(str(grid.cv_results_).replace(\", '\", \",\\n'\") + '\\n')\n",
    "  fh.write(str(grid.best_score_) + '\\n')  \n",
    "  fh.write(str(grid.best_estimator_.alpha) + '\\n')\n",
    "  fh.close()\n",
    "\n",
    "  # MODEL: BEST\n",
    "  best = grid.best_estimator_\n",
    "\n",
    "  best.fit(X, y)\n",
    "  joblib.dump(best, 'best.{}.pkl'.format(model[1])) # pickle\n",
    "\n",
    "  best_performance = BinaryClassificationPerformance(best.predict(X), y, 'best')\n",
    "  best_performance.compute_measures()\n",
    "  print(best_performance.performance_measures)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pg = {'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "'hidden_layer_sizes': [(100,1), (100,2), (100,3)],\n",
    "#'alpha': [10.0 ** -np.arange(1, 7)],\n",
    "'alpha': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "'activation': [\"logistic\", \"relu\", \"Tanh\"],\n",
    "'tol': [1e-2, 1e-4, 1e-6],\n",
    "'epsilon': [1e-3, 1e-7, 1e-8, 1e-9, 1e-8]\n",
    "}\n",
    "\n",
    "fh = open(\"GridSearch.txt\", \"a\")\n",
    "grid = GridSearchCV(estimator=mlp, param_grid=pg, n_jobs=2) #\n",
    "grid.fit(X, y)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.cv_results_)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)\n",
    "\n",
    "fh.write('\\n########\\n')\n",
    "fh.write(str(datetime.datetime.now()))\n",
    "fh.write('\\n########\\n')\n",
    "fh.write(str(model) + '\\n')  \n",
    "fh.write(str(grid.cv_results_).replace(\", '\", \",\\n'\") + '\\n')\n",
    "fh.write(str(grid.best_score_) + '\\n')  \n",
    "fh.write(str(grid.best_estimator_.alpha) + '\\n')\n",
    "fh.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# MODEL: BEST\n",
    "best = grid.best_estimator_\n",
    "\n",
    "best.fit(X, y)\n",
    "joblib.dump(best, 'best.pkl') # pickle\n",
    "\n",
    "best_performance = BinaryClassificationPerformance(best.predict(X), y, 'best')\n",
    "best_performance.compute_measures()\n",
    "print(best_performance.performance_measures)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### ROC plot to compare performance of various models and fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if isTraining:\n",
    "    fList = []\n",
    "else:\n",
    "    fits = [svm_performance, lgs_performance, nbs_performance, rdg_performance, prc_performance]\n",
    "    fList = fits\n",
    "\n",
    "for fit in fList:\n",
    "    plt.plot(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], 'ro')\n",
    "    plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], fit.desc)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "if isTraining:\n",
    "    plt.title('ROC plot: training set')\n",
    "else:\n",
    "    plt.title('ROC plot: testing set')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "papermill": {
   "environment_variables": {},
   "output_path": "output.ipynb",
   "parameters": null,
   "version": "0.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}