{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.000516,
     "end_time": "2018-04-10T22:58:15.882987",
     "exception": false,
     "start_time": "2018-04-10T22:58:15.882471",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.286471,
     "end_time": "2018-04-10T22:58:16.178460",
     "exception": false,
     "start_time": "2018-04-10T22:58:15.891989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "%matplotlib inline   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.018653,
     "end_time": "2018-04-10T22:58:16.197243",
     "exception": false,
     "start_time": "2018-04-10T22:58:16.178590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.system('ps aux | grep wolfm2')\n",
    "#os.system('killall -s SIGKILL -u wolfm2')\n",
    "#os.system('cp /home/wolfm2/job.sh .; echo test 1>&2') #; cp ../job.log ../jerbb.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 6e-06,
     "end_time": "2018-04-10T22:58:16.197288",
     "exception": false,
     "start_time": "2018-04-10T22:58:16.197282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Read raw training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 1.38919,
     "end_time": "2018-04-10T22:58:17.595400",
     "exception": false,
     "start_time": "2018-04-10T22:58:16.206210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318500, 14)\n"
     ]
    }
   ],
   "source": [
    "amazon = pd.read_csv('/home/wolfm2/amazon_data.0/raw_data_train.csv')\n",
    "#amazon = pd.read_csv('/home/eydu/amazon_data/raw_data_train.csv')\n",
    "#amazon = pd.read_csv('/home/ich/amazon_data/raw_data_train.csv')\n",
    "\n",
    "print(amazon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.012263,
     "end_time": "2018-04-10T22:58:17.607756",
     "exception": false,
     "start_time": "2018-04-10T22:58:17.595493",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 0.1      Id   ProductId          UserId  \\\n",
      "0       39590        218604  218605  B003TWBUWI  A3W25DGQAN3BCA   \n",
      "1      366204        203525  203526  B000CQC04Q  A3QLP6M4RIT77H   \n",
      "2      292985        358338  358339  B001EO5Y7K  A1Q7A78VSQ5GQ4   \n",
      "3      425853        203051  203052  B000KEVF1O   A6NAF9GQHKOVO   \n",
      "4      205723        483843  483844  B003XDH6M6  A1X8BQQGCEHQ1V   \n",
      "\n",
      "                       ProfileName  HelpfulnessNumerator  \\\n",
      "0                          glouise                     1   \n",
      "1                       Snow Bound                     1   \n",
      "2  Nice Lady \"a reasonable person\"                     0   \n",
      "3                   KlutinaQuilter                     0   \n",
      "4   Kim Cantrell \"Soap Box Bandit\"                     1   \n",
      "\n",
      "   HelpfulnessDenominator  Score        Time  \\\n",
      "0                       1      4  1341705600   \n",
      "1                       1      5  1267228800   \n",
      "2                       0      5  1325980800   \n",
      "3                       0      5  1310342400   \n",
      "4                       2      2  1318291200   \n",
      "\n",
      "                                             Summary  \\\n",
      "0                            4 with reservations....   \n",
      "1                         I'm in LOVE with this tea!   \n",
      "2  Have tried many varieties of Turkish delight. ...   \n",
      "3                      Greenies, how can you say no?   \n",
      "4                   Stays With You Long After Eating   \n",
      "\n",
      "                                                Text  helpScore  helpful  \n",
      "0  Okay, these were very moist and they did have ...        1.0    False  \n",
      "1  It is sinfully delicious and a nice way to end...        1.0    False  \n",
      "2  This Turkish Delight is soft and delectable. A...        NaN    False  \n",
      "3  Of course dogs love Greenies!  I was happy to ...        NaN    False  \n",
      "4  I love Pomegranate.  I love licorice.  I love ...        0.5    False  \n",
      "0.0732025117739\n"
     ]
    }
   ],
   "source": [
    "print(amazon.head())\n",
    "print(amazon['helpful'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 8e-06,
     "end_time": "2018-04-10T22:58:17.608075",
     "exception": false,
     "start_time": "2018-04-10T22:58:17.608067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Feature extraction on natural language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.013312,
     "end_time": "2018-04-10T22:58:17.631242",
     "exception": false,
     "start_time": "2018-04-10T22:58:17.617930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "# corpus = amazon.Text.as_matrix()\n",
    "# X_bag_of_words = vectorizer.fit_transform(corpus)\n",
    "# print(X_bag_of_words.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 0.20232,
     "end_time": "2018-04-10T22:58:17.833596",
     "exception": false,
     "start_time": "2018-04-10T22:58:17.631276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('popular')\n",
    "\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 295.626077,
     "end_time": "2018-04-10T23:03:13.459769",
     "exception": false,
     "start_time": "2018-04-10T22:58:17.833692",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolfm2/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/home/wolfm2/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolfm2/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/home/wolfm2/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wolfm2/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n",
      "/home/wolfm2/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/hashing.py:94: DeprecationWarning: the option non_negative=True has been deprecated in 0.19 and will be removed in version 0.21.\n",
      "  \" in version 0.21.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(318500, 1048576)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEICAYAAACj2qi6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nadyr++UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff9312a6630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vectorize Bag of Words from review text; as sparse matrix\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# amazon['reviewLen'] = amazon['Text'].str.len() # Use this later /X as gross dummy\n",
    "\n",
    "amazon['textLower'] = amazon['Text'].str.lower()\n",
    "# look at the text strip_accents=ascii,  stop_words={'english'}, token_pattern = r'\\b[a-zA-Z0-9]{3,}\\b'),\n",
    "hv0 = HashingVectorizer(n_features=2 ** 19, non_negative=True, tokenizer=LemmaTokenizer(), ngram_range=(1,3))\n",
    "X_hv0 = hv0.fit_transform(amazon.textLower)\n",
    "\n",
    "\n",
    "amazon['summaryFilter'] = amazon['Summary'].apply(lambda x: \" \" if x is np.nan else x) # some were np.nans\n",
    "amazon['sfLower'] = amazon['summaryFilter'].str.lower()\n",
    "# # and a second domain where we look at the summary\n",
    "hv1 = HashingVectorizer(n_features=2 ** 19, non_negative=True, tokenizer=LemmaTokenizer(), ngram_range=(1,3))\n",
    "X_hv1 = hv1.fit_transform(amazon.sfLower) \n",
    "\n",
    "# Another hash domain we want to count but not scale\n",
    "# amazon['timeFilter'] = amazon['Time'].apply(lambda x: str(int(x)%(86400 * 7))) # converts to day of week\n",
    "# hv2 = HashingVectorizer(n_features=2 ** 17, non_negative=True, strip_accents=ascii, \n",
    "#                            ngram_range=(1,1)) \n",
    "# X_hv2 = hv2.fit_transform(amazon.timeFilter + \" \" + amazon.ProductId + \" \" + amazon.UserId) # mw adds uid as token\n",
    "\n",
    "amazon['logReviewLen'] = np.round(np.log(amazon['Text'].str.len()),decimals=1) + 10\n",
    "amazon.hist(column=\"logReviewLen\")\n",
    "\n",
    "amazon['ScoreX'] = amazon['Score'].apply(lambda x: str(x)) # make score acceptable\n",
    "amazon['sLogReviewLen'] = amazon['logReviewLen'].apply(lambda x: str(x)) # make score acceptable\n",
    "hv2 = HashingVectorizer(n_features=2 ** 17, non_negative=True, ngram_range=(1,1)) \n",
    "X_hv2 = hv2.fit_transform(amazon.ScoreX + \" \" + amazon.UserId + \" \" + amazon.sLogReviewLen) # mw adds uid as token\n",
    "\n",
    "import scipy.sparse as sp\n",
    "X_hv = sp.hstack([X_hv0, X_hv1], format='csr')\n",
    "print(X_hv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.01255,
     "end_time": "2018-04-10T23:03:13.472728",
     "exception": false,
     "start_time": "2018-04-10T23:03:13.460178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x = amazon.UserId + \" \" +  amazon.Text\n",
    "# x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.012628,
     "end_time": "2018-04-10T23:03:13.485495",
     "exception": false,
     "start_time": "2018-04-10T23:03:13.472867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hv2.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to be able to use this model fit on other data (the test set)\n",
    "# So let's save a copy of this instance of HashingVectorizer to be able to transform other data with this fit\n",
    "# http://scikit-learn.org/stable/modules/model_persistence.html\n",
    "joblib.dump(hv0, 'hv0.pkl') # pickle\n",
    "joblib.dump(hv1, 'hv1.pkl') # pickle\n",
    "joblib.dump(hv2, 'hv2.pkl') # pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 19.038588,
     "end_time": "2018-04-10T23:03:32.524350",
     "exception": false,
     "start_time": "2018-04-10T23:03:13.485762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X_hv)\n",
    "\n",
    "joblib.dump(transformer, 'transformer.pkl') # pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.016195,
     "end_time": "2018-04-10T23:03:32.540655",
     "exception": false,
     "start_time": "2018-04-10T23:03:32.524460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": 7e-06,
     "end_time": "2018-04-10T23:03:32.540716",
     "exception": false,
     "start_time": "2018-04-10T23:03:32.540709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create additional quantitative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 12.199851,
     "end_time": "2018-04-10T23:03:44.751681",
     "exception": false,
     "start_time": "2018-04-10T23:03:32.551830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# features from Amazon.csv to add to feature set\n",
    "import re\n",
    "\n",
    "#amazon['reviewLen'] = amazon['Text'].str.len()\n",
    "#amazon['summaryLen'] = amazon['summaryFilter'].str.len()\n",
    "\n",
    "#amazon['rlMeanDist'] = amazon['reviewLen'].apply(lambda x: abs(x-80)) # 80 is avg summary len. Thx George!\n",
    "#amazon['slMeanDist'] = amazon['summaryLen'].apply(lambda x: abs(x-8)) # 8. just guessing here.\n",
    "\n",
    "#import zlib\n",
    "#amazon['nameHash'] = zlib.crc32(str(amazon['UserId']).encode('utf8'))\n",
    "#amazon['nameHash'] = amazon['UserId'].apply(lambda x: zlib.crc32(str(x).encode('utf8'))) # bad. don't do it this way\n",
    "\n",
    "# stackoverflow.com/questions/15772371/finding-average-length-of-items-in-a-list-python\n",
    "# averages array element lengths\n",
    "def avgLen(text, regex):\n",
    "    lst = re.findall(regex, text)\n",
    "    lengths = [len(i) for i in lst]\n",
    "    return 0 if len(lengths) == 0 else (float(sum(lengths)) / len(lengths)) \n",
    "\n",
    "# ratio of regex to whole\n",
    "def cRatio(text, regex):\n",
    "    num = len(re.findall(regex, text))\n",
    "    text = \"\" if text is np.nan else text\n",
    "    den = len(text)\n",
    "    return 0 if den == 0 else num / den\n",
    "\n",
    "# Review Len\n",
    "amazon['summaryLen'] = amazon['Text'].str.len()\n",
    "\n",
    "# Num Words\n",
    "amazon['numWords'] = amazon['Text'].apply(lambda x: len(re.findall(\"[a-zA-Z']+\", x)))\n",
    "\n",
    "# Num Cap Words\n",
    "amazon['numCapWords'] = amazon['Text'].apply(lambda x: len(re.findall(\"[A-Z']+\", x)))\n",
    "\n",
    "# Avg Sentence Len\n",
    "amazon['avgSenLen'] = amazon['Text'].apply(lambda x: avgLen(x, \"[a-zA-Z' ]+\"))\n",
    "\n",
    "# Avg Word Len\n",
    "amazon['avgWrdLen'] = amazon['Text'].apply(lambda x: avgLen(x, \"[a-zA-Z']+\"))\n",
    "\n",
    "# ! Ratio\n",
    "amazon['ratioBang'] = amazon['Text'].apply(lambda x: cRatio(x, \"\\!\"))\n",
    "\n",
    "# ? Ratio                             \n",
    "amazon['ratioQmark'] = amazon['Text'].apply(lambda x: cRatio(x, \"\\?\"))\n",
    "\n",
    "# X_quant_features = amazon[[\"Score\", \"reviewLen\", \"summaryLen\", \"rlMeanDist\", \"slMeanDist\"]]\n",
    "# print(X_quant_features.head(10))\n",
    "# print(type(X_quant_features))\n",
    "X_quant_features = amazon[['summaryLen', 'numWords', 'numCapWords', 'avgSenLen', 'avgWrdLen', 'ratioBang', 'ratioQmark']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 7e-06,
     "end_time": "2018-04-10T23:03:44.751789",
     "exception": false,
     "start_time": "2018-04-10T23:03:44.751782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Combine all quantitative features into a single sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "X_quant_features_csr = csr_matrix(X_quant_features)\n",
    "X_combined = hstack([X_tfidf, X_quant_features_csr, X_hv2])  # we dont want to penalize hv2 w tfidf MW\n",
    "X_matrix = csr_matrix(X_combined) # convert to sparse matrix\n",
    "print(X_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Create `X`, scaled matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(with_mean=False)\n",
    "X = sc.fit_transform(X_matrix)\n",
    "print(X.shape)\n",
    "\n",
    "joblib.dump(sc, 'sc.pkl') # pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### create `y`, vector of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = amazon['helpful'].values\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from my_measures import BinaryClassificationPerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: SVM, linear\n",
    "# from sklearn import linear_model\n",
    "# svm = linear_model.SGDClassifier()\n",
    "# svm.fit(X, y)\n",
    "# joblib.dump(svm, 'svm.pkl') # pickle\n",
    "\n",
    "# svm_performance = BinaryClassificationPerformance(svm.predict(X), y, 'svm')\n",
    "# svm_performance.compute_measures()\n",
    "# print(svm_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: logistic regression\n",
    "# from sklearn import linear_model\n",
    "# #lgs = linear_model.SGDClassifier(loss='log', n_iter=50, alpha=0.00001)\n",
    "# lgs = linear_model.SGDClassifier(loss='log', n_iter=1000, alpha=0.1)\n",
    "\n",
    "# lgs.fit(X, y)\n",
    "# joblib.dump(lgs, 'lgs.pkl') # pickle\n",
    "\n",
    "# lgs_performance = BinaryClassificationPerformance(lgs.predict(X), y, 'lgs')\n",
    "# lgs_performance.compute_measures()\n",
    "# print(lgs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: Naive Bayes\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# nbs = MultinomialNB()\n",
    "# nbs.fit(X, y)\n",
    "# joblib.dump(nbs, 'nbs.pkl') # pickle\n",
    "\n",
    "# nbs_performance = BinaryClassificationPerformance(nbs.predict(X), y, 'nbs')\n",
    "# nbs_performance.compute_measures()\n",
    "# print(nbs_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: Ridge Regression Classifier\n",
    "# from sklearn import linear_model\n",
    "# rdg = linear_model.RidgeClassifier()\n",
    "# rdg.fit(X, y)\n",
    "# joblib.dump(rdg, 'rdg.pkl') # pickle\n",
    "\n",
    "# rdg_performance = BinaryClassificationPerformance(rdg.predict(X), y, 'rdg')\n",
    "# rdg_performance.compute_measures()\n",
    "# print(rdg_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # MODEL: Perceptron\n",
    "# from sklearn import linear_model\n",
    "# prc = linear_model.SGDClassifier(loss='perceptron')\n",
    "# prc.fit(X, y)\n",
    "# joblib.dump(prc, 'prc.pkl') # pickle\n",
    "\n",
    "# prc_performance = BinaryClassificationPerformance(prc.predict(X), y, 'prc')\n",
    "# prc_performance.compute_measures()\n",
    "# print(prc_performance.performance_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier # mw\n",
    "\n",
    "# prepare a range of alpha values to test\n",
    "# alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "alphas = np.array([1, 0.1, 0.01, 0.001, 0.0001, 0.00001])\n",
    "Cs = np.array([0.001, 0.01, 0.1, 1, 10, 100, 1000])\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "# model = linear_model.SGDClassifier(loss='perceptron', max_iter=50) # max_iter 1000\n",
    "\n",
    "mlp = MLPClassifier(random_state=0)\n",
    "svm = linear_model.SGDClassifier(n_iter=500)\n",
    "lgs = linear_model.SGDClassifier(loss='log', n_iter=500)\n",
    "nbs = MultinomialNB()\n",
    "rdg = linear_model.RidgeClassifier()\n",
    "prc = linear_model.SGDClassifier(loss='perceptron', n_iter=500)\n",
    "\n",
    "for model in [[svm,\"svm\"], [lgs,\"lgs\"], [prc,\"prc\"], [nbs,\"nbs\"], [rdg,\"rdg\"]]: \n",
    "# for model in []: \n",
    "# for model in [rdg]:    \n",
    "  fh = open(\"GridSearch.txt\", \"a\")\n",
    "  grid = GridSearchCV(estimator=model[0], param_grid=dict(alpha=alphas), n_jobs=2) #\n",
    "  grid.fit(X, y)\n",
    "  print(grid)\n",
    "  # summarize the results of the grid search\n",
    "  print(grid.cv_results_)\n",
    "  print(grid.best_score_)\n",
    "  print(grid.best_estimator_.alpha)\n",
    "\n",
    "  fh.write('\\n########\\n')\n",
    "  fh.write(str(datetime.datetime.now()))\n",
    "  fh.write('\\n########\\n')\n",
    "  fh.write(str(model[0]) + '\\n')  \n",
    "  fh.write(str(grid.cv_results_).replace(\", '\", \",\\n'\") + '\\n')\n",
    "  fh.write(str(grid.best_score_) + '\\n')  \n",
    "  fh.write(str(grid.best_estimator_.alpha) + '\\n')\n",
    "  fh.close()\n",
    "\n",
    "  # MODEL: BEST\n",
    "  best = grid.best_estimator_\n",
    "\n",
    "  best.fit(X, y)\n",
    "  joblib.dump(best, 'best.{}.pkl'.format(model[1])) # pickle\n",
    "\n",
    "  best_performance = BinaryClassificationPerformance(best.predict(X), y, 'best')\n",
    "  best_performance.compute_measures()\n",
    "  print(best_performance.performance_measures)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "pg = {'learning_rate': [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "'hidden_layer_sizes': [(100,1), (100,2), (100,3)],\n",
    "#'alpha': [10.0 ** -np.arange(1, 7)],\n",
    "'alpha': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "'activation': [\"logistic\", \"relu\", \"Tanh\"],\n",
    "'tol': [1e-2, 1e-4, 1e-6],\n",
    "'epsilon': [1e-3, 1e-7, 1e-8, 1e-9, 1e-8]\n",
    "}\n",
    "\n",
    "fh = open(\"GridSearch.txt\", \"a\")\n",
    "grid = GridSearchCV(estimator=mlp, param_grid=pg, n_jobs=2) #\n",
    "grid.fit(X, y)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.cv_results_)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)\n",
    "\n",
    "fh.write('\\n########\\n')\n",
    "fh.write(str(datetime.datetime.now()))\n",
    "fh.write('\\n########\\n')\n",
    "fh.write(str(model) + '\\n')  \n",
    "fh.write(str(grid.cv_results_).replace(\", '\", \",\\n'\") + '\\n')\n",
    "fh.write(str(grid.best_score_) + '\\n')  \n",
    "fh.write(str(grid.best_estimator_.alpha) + '\\n')\n",
    "fh.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# MODEL: BEST\n",
    "best = grid.best_estimator_\n",
    "\n",
    "best.fit(X, y)\n",
    "joblib.dump(best, 'best.pkl') # pickle\n",
    "\n",
    "best_performance = BinaryClassificationPerformance(best.predict(X), y, 'best')\n",
    "best_performance.compute_measures()\n",
    "print(best_performance.performance_measures)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### ROC plot to compare performance of various models and fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #fits = [svm_performance, lgs_performance, nbs_performance, rdg_performance, prc_performance]\n",
    "# fits = [svm_performance, lgs_performance, rdg_performance, prc_performance]\n",
    "\n",
    "# for fit in fits:\n",
    "#     plt.plot(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "#              fit.performance_measures['TP'] / fit.performance_measures['Pos'], 'ro')\n",
    "#     plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "#              fit.performance_measures['TP'] / fit.performance_measures['Pos'], fit.desc)\n",
    "# plt.axis([0, 1, 0, 1])\n",
    "# plt.title('ROC plot: training set')\n",
    "# plt.xlabel('False positive rate')\n",
    "# plt.ylabel('True positive rate')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "papermill": {
   "environment_variables": {},
   "output_path": "output.ipynb",
   "parameters": null,
   "version": "0.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}